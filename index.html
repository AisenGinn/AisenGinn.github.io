<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Changhe Chen - Personal Website</title>
    <link rel="stylesheet" href="style.css">
    <script defer src="script.js"></script>
</head>
<body>
    <header>
        <h1>Changhe Chen</h1>
        <p>MS in Robotics | University of Michigan</p>
        <nav>
            <ul>
                <li><a href="#education">Education</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#research">Research</a></li>
                <li><a href="#work">Work Experience</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#skills">Skills</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>
    </header>

    <section id="education">
        <h2>Education</h2>
        <p><strong>University of Michigan, Ann Arbor</strong> | MS in Robotics (Aug 2024 - Present)</p>
        <p><strong>University of Toronto</strong> | BASc in Electrical and Computer Engineering (2019 - 2024)</p>
    </section>

    <section id="publications">
        <h2>Publications</h2>
        <div class="publication">
            <img src="icra2024_cover.jpg" alt="ICRA 2024 Paper Cover">
            <div>
                <h3>CRITERIA: A New Benchmarking Paradigm to Evaluate Trajectory Prediction Approaches</h3>
                <p><strong><span class="highlight">Changhe Chen</span>, Mozhgan Pourkeshavarz, Amir Rasouli</strong></p>
                <p>IEEE International Conference on Robotics and Automation (ICRA), 2024</p>
                <a href="https://arxiv.org/abs/2310.07794">[Paper]</a> <a href="https://github.com/huawei-noah/SMARTS/tree/CRITERIA-latest/papers/CRITERIA">[Code]</a> <a href="https://www.argoverse.org/av1.html">[Dataset]</a>
            </div>
        </div>
        <div class="publication">
            <img src="tns2023_cover.jpg" alt="TNS 2023 Paper Cover">
            <div>
                <h3>Using Upsampling Conv-LSTM for Respiratory Sound Classification</h3>
                <p><strong><span class="highlight">Changhe Chen</span>, Rongbo Zhang</strong></p>
                <p>Theoretical and Natural Science (TNS), 2023</p>
                <a href="https://www.ewadirect.com/proceedings/tns/article/view/9298">[Paper]</a> <a href="https://github.com/AisenGinn/BioCAS_clean">[Code]</a> <a href="https://github.com/SJTU-YONGFU-RESEARCH-GRP/SPRSound-Contest">[Dataset]</a>
            </div>
        </div>
        <div class="publication">
            <img src="iccv2023_cover.jpg" alt="ICCV 2023 Paper Cover">
            <div>
                <h3>Learn TAROT with MENTOR: A Meta-Learned Self-Supervised Approach for Trajectory Prediction</h3>
                <p><strong>Mozhgan Pourkeshavarz, <span class="highlight">Changhe Chen</span>, Amir Rasouli</strong></p>
                <p>The International Conference on Computer Vision (ICCV), 2023</p>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pourkeshavarz_Learn_TAROT_with_MENTOR_A_Meta-Learned_Self-Supervised_Approach_for_Trajectory_ICCV_2023_paper.pdf">[Paper]</a> <a href="https://www.argoverse.org/av1.html">[Dataset]</a>
            </div>
        </div>
    </section>

    <section id="research">
        <h2>Research Experience</h2>
        <div class="research">
            <h3>AI-Driven Robotic Structure Evaluation (University of Michigan - HDR Lab)</h3>
            <p>Developed a data generation pipeline in Evogym for evaluating robotic structures.</p>
            <p>Utilized large language models (LLMs) to optimize robotic structures through AI insights.</p>
        </div>
        <div class="research">
            <h3>Robot Skill Learning via Large-Scale Video Representation (University of Michigan - MMint Lab)</h3>
            <p>Designed a robot task-learning system integrating multi-modal VLMs for improved task execution.</p>
            <p>Implemented instruction-based semantic segmentation to enhance robot perception.</p>
        </div>
    </section>

    <section id="skills">
        <h2>Skills</h2>
        <p><strong>Programming:</strong> Python (PyTorch, TensorFlow), C/C++, MATLAB, Java, Verilog, ARM Assembly</p>
        <p><strong>Software Tools:</strong> ROS1/2, RL (DDPG, RGCN), SMARTS Simulation, Deep Graph Library</p>
    </section>

    <section id="contact">
        <h2>Contact</h2>
        <p>Email: <a href="mailto:changhec@umich.edu">changhec@umich.edu</a></p>
        <p>GitHub: <a href="https://github.com/AisenGinn">github.com/AisenGinn</a></p>
    </section>

    <footer>
        <p>&copy; 2025 Changhe Chen</p>
    </footer>
</body>
</html>
