<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Changhe Chen - Personal Website</title>
    <link rel="stylesheet" href="style.css">
    <script defer src="script.js"></script>
</head>
<body>
    <header>
        <h1>Changhe Chen</h1>
        <p>MS in Robotics | University of Michigan</p>
        <nav>
            <ul>
                <li><a href="#aboutme">About Me</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#research">Research</a></li>
                <li><a href="#work">Work Experience</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#skills">Skills</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>
    </header>

    <section id="Bio">
        <h2>Bio</h2>
        <p>
            Changhe Chen is a master's student in Robotics at the <a href="https://robotics.umich.edu/" target="_blank">University of Michigan</a>, 
            (advised by Prof. Nima Fazeli and Prof. Xiaonan (Sean) Huang). Prior to joining the University of Michigan, 
            Changhe received his BASc in Electrical Engineering from the  <a href="https://www.ece.utoronto.ca/" target="_blank">University of Toronto</a>, 
            where he worked with Prof. Chan Carusone on reinforcement learning for analog circuit design. During his undergraduate studies, 
            he also interned as a research assistant at Huawei's Noah's Ark Lab, developing multi-agent reinforcement learning platforms and
             advanced trajectory prediction models for autonomous driving.In summer 2025, Changhe enjoyed collaborating on robotics research 
             and real-world robotic systems with <a href="https://computationalrobotics.seas.harvard.edu/" target="_blank">Heng Yang's group</a> 
             at <a href="https://seas.harvard.edu/robotics" target="_blank">Harvard University</a>. Changhe's current research focuses on embodied AI, 
             data-efficient robot learning, and learning from humans, with an emphasis on developing multi-modal visual language models 
             and instruction-based semantic segmentation to enable robots to better understand and execute complex tasks. 
        </p>
    </section>

    <section id="publications">
        <h2>Publications</h2>
        <p style="color:black; font-style:italic; font-size:0.9em;">* indicates equal contribution.</p>
        <div class="publication">
            <img src="figures/paper_figures/MoTVLA.png" alt="MoTVLA Paper Cover">
            <div>
                <h3>MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning</h3>
                <p><strong>Wenhui Huang*, <span class="highlight">Changhe Chen*</span>, Han Qi, Lv Chen, Yilun Du, Heng Yang</strong></p>
                <p>arXiv preprint arXiv:2509.16053</p>
                <a href="https://motvla.github.io/MoTVLA-website/">[Website]</a>
            </div>
        </div>
        <div class="publication">
            <img src="figures/paper_figures/compose.png" alt="Compose Paper Cover">
            <div>
                <h3>Compose by Focus: Scene Graph-based Atomic Skills</h3>
                <p><strong>Han Qi, <span class="highlight">Changhe Chen</span>, Heng Yang</strong></p>
                <p>arXiv preprint arXiv:2509.16053</p>
                <a href="https://arxiv.org/pdf/2509.16053">[Paper]</a> <a href="https://computationalrobotics.seas.harvard.edu/SkillComposition/">[Website]</a>
            </div>
        </div>
        <div class="publication">
            <img src="figures/paper_figures/ViSA.png" alt="ViSA Paper Cover">
            <div>
                <h3>ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow</h3>
                <p><strong><span class="highlight">Changhe Chen*</span>, Quantao Yang*, Xiaohao Xu, Nima Fazeli, Olov Andersson</strong></p>
                <p>arXiv preprint arXiv:2505.01288</p>
                <a href="https://arxiv.org/abs/2505.01288">[Paper]</a> <a href="https://visaflow-web.github.io/ViSAFLOW/">[Website]</a>
            </div>
        </div>
        <div class="publication">
            <img src="figures/paper_figures/robocrafter.png" alt="RoboCrafter Paper Cover">
            <div>
                <h3>Large Language Models as Natural Selector for Embodied Soft Robot Design</h3>
                <p><strong><span class="highlight">Changhe Chen</span>, Xiaohao Xu, Xiangdong Wang, Xiaonan Huang</strong></p>
                <p>arXiv preprint arXiv:2503.02249 </p>
                <a href="https://arxiv.org/abs/2503.02249">[Paper]</a> <a href="https://github.com/AisenGinn/evogym_data_generation">[Code]</a> <a href="https://github.com/AisenGinn/evogym_data_generation">[Dataset]</a>
            </div>
        </div>
        <div class="publication">
            <img src="figures/paper_figures/icra2024_cover.jpg" alt="ICRA 2024 Paper Cover">
            <div>
                <h3>CRITERIA: A New Benchmarking Paradigm to Evaluate Trajectory Prediction Approaches</h3>
                <p><strong><span class="highlight">Changhe Chen</span>, Mozhgan Pourkeshavarz, Amir Rasouli</strong></p>
                <p>IEEE International Conference on Robotics and Automation (ICRA), 2024</p>
                <a href="https://arxiv.org/abs/2310.07794">[Paper]</a> <a href="https://github.com/huawei-noah/SMARTS/tree/CRITERIA-latest/papers/CRITERIA">[Code]</a> <a href="https://www.argoverse.org/av1.html">[Dataset]</a>
            </div>
        </div>
        <div class="publication">
            <img src="figures/paper_figures/tns2023_cover.jpg" alt="TNS 2023 Paper Cover">
            <div>
                <h3>Using Upsampling Conv-LSTM for Respiratory Sound Classification</h3>
                <p><strong><span class="highlight">Changhe Chen</span>, Rongbo Zhang</strong></p>
                <p>Theoretical and Natural Science (TNS), 2023</p>
                <a href="https://www.ewadirect.com/proceedings/tns/article/view/9298">[Paper]</a> <a href="https://github.com/AisenGinn/BioCAS_clean">[Code]</a> <a href="https://github.com/SJTU-YONGFU-RESEARCH-GRP/SPRSound-Contest">[Dataset]</a>
            </div>
        </div>
        <div class="publication">
            <img src="figures/paper_figures/iccv2023_cover.jpg" alt="ICCV 2023 Paper Cover">
            <div>
                <h3>Learn TAROT with MENTOR: A Meta-Learned Self-Supervised Approach for Trajectory Prediction</h3>
                <p><strong>Mozhgan Pourkeshavarz, <span class="highlight">Changhe Chen</span>, Amir Rasouli</strong></p>
                <p>The International Conference on Computer Vision (ICCV), 2023</p>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pourkeshavarz_Learn_TAROT_with_MENTOR_A_Meta-Learned_Self-Supervised_Approach_for_Trajectory_ICCV_2023_paper.pdf">[Paper]</a> <a href="https://www.argoverse.org/av1.html">[Dataset]</a>
            </div>
        </div>
    </section>

    <section id="research">
        <h2>Research Experience</h2>
        <div class="research">
            <h3>Visiting Fellow, Computational Robotics Group (Harvard University), Advised by Prof. Heng Yang</h3>
            <p>Designed MoTVLA, a mixture-of-transformers framework unifying generalist vision-language reasoning with fast motion decomposition for robotic manipulation.</p>
            <p>Developed diffusion-policy methods to enhance language steerability and inference speed in robot skill learning.</p>
            <p>Explored scene graph-based visuomotor policies integrating GNNs and diffusion learning for robust, long-horizon skill composition under visual distractors.</p>
        </div>
        <div class="research">
            <h3>AI-Driven Robotic Structure Evaluation (University of Michigan - HDR Lab), Advised by Prof. Xiaonan (Sean) Huang</h3>
            <p>Developed a data generation pipeline in Evogym for evaluating robotic structures.</p>
            <p>Utilized large language models (LLMs) to optimize robotic structures through AI insights.</p>
            <p>Enhanced robotic simulation workflows by combining AI-driven evaluations with structured data generation for improved design and performance analysis.</p>
        </div>
        <div class="research">
            <h3>Robot Skill Learning via Large-Scale Video Representation (University of Michigan - MMint Lab), Advised by Prof. Nima Fazeli</h3>
            <p>Designed a robot task-learning system integrating multi-modal VLMs for improved task execution.</p>
            <p>Implemented instruction-based semantic segmentation to enhance robot perception.</p>
        </div>
        <div class="research">
            <h3>Transistor Sizing and Optimization for Low-dropout Circuit by RL (University of Toronto), Advised by Prof. Chan Carusone</h3>
            <p>Led a team to develop an RL algorithm optimizing circuit parameters using Cadence and TSMC 65nm PDK.</p>
            <p>Used RGCN and DDPG to encode circuit topology and optimize design cycles by 80%.</p>
        </div>
    </section>

    <section id="work">
        <h2>Work Experience</h2>
        <div class="work">
            <h3>Huawei Technologies - Research Assistant at Noah's Ark Lab</h3>
            <p><strong>Multi-Agents RL Simulation Platform “SMARTS”</strong></p>
            <ul>
                <li>Upgraded SMARTS, resolving 20+ maintenance and improvement issues.</li>
                <li>Helped organize autonomous driving competitions using SMARTS.</li>
            </ul>
            <p><strong>Autonomous Driving Trajectory Prediction Models</strong></p>
            <ul>
                <li>Used Deep Graph Library to improve scene encoding for Argoverse dataset, achieving 20% SOTA improvement.</li>
                <li>Proposed new diversity and admissibility metrics for trajectory prediction, and conducted scene classification based on road structure, models' performance, and data properties on the Argoverse dataset.</li>
                <li>Used proposed metrics and benchmarks to conduct experiments on representative prediction models. Produced accurate ranking of models and characterized models' behavior.</li>
            </ul>
        </div>
    </section>

    <section id="projects">
        <h2>Leadership & Projects</h2>
        <div class="activity">
            <h3>U of T's Autodrive Team (aUToronto)</h3>
            <p>Designed radar-based autonomous vehicle systems; 1st place in SAE Autodrive Challenge II.</p>
        </div>
        <div class="activity">
            <h3>IEEE BioCAS Grand Challenge</h3>
            <p>Led a team to developed an ML-based respiratory sound classification model, reaching 75% accuracy.</p>
        </div>
        <div class="activity">
            <h3>Street Map Navigation System</h3>
            <p>Led a team to built a C++ navigation app using OpenStreetMap API with a responsive UI.</p>
        </div>
    </section>

    <section id="skills">
        <h2>Skills</h2>
        <p><strong>Programming:</strong> Python (PyTorch, TensorFlow), C/C++, MATLAB, Java, Verilog, ARM Assembly</p>
        <p><strong>Software Tools:</strong> ROS1/2, RL (DDPG, RGCN), SMARTS Simulation, Deep Graph Library</p>
    </section>

    <section id="contact">
        <h2>Contact</h2>
        <p>Email: <a href="mailto:changhec@umich.edu">changhec@umich.edu</a></p>
        <p>GitHub: <a href="https://github.com/AisenGinn">github.com/AisenGinn</a></p>
    </section>

    <footer>
        <p>&copy; 2025 Changhe Chen</p>
    </footer>
</body>
</html>
